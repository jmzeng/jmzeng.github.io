@misc{bhanot_downstream_2022,
 abstract = {This paper evaluates synthetically generated healthcare data for biases and investigates the effect of fairness mitigation techniques on utility-fairness. Privacy laws limit access to health data such as Electronic Medical Records (EMRs) to preserve patient privacy. Albeit essential, these laws hinder research reproducibility. Synthetic data is a viable solution that can enable access to data similar to real healthcare data without privacy risks. Healthcare datasets may have biases in which certain protected groups might experience worse outcomes than others. With the real data having biases, the fairness of synthetically generated health data comes into question. In this paper, we evaluate the fairness of models generated on two healthcare datasets for gender and race biases. We generate synthetic versions of the dataset using a Generative Adversarial Network called HealthGAN, and compare the real and synthetic model's balanced accuracy and fairness scores. We find that synthetic data has different fairness properties compared to real data and fairness mitigation techniques perform differently, highlighting that synthetic data is not bias free.},
 author = {Bhanot, Karan and Baldini, Ioana and Wei, Dennis and Zeng, Jiaming and Bennett, Kristin P.},
 copyright = {All rights reserved},
 file = {arXiv Fulltext PDF:/Users/jiaming/Zotero/storage/MNB3UBEK/Bhanot et al. - 2022 - Downstream Fairness Caveats with Synthetic Healthc.pdf:application/pdf;arXiv.org Snapshot:/Users/jiaming/Zotero/storage/XKTCHUL5/2203.html:text/html},
 keywords = {Computer Science - Machine Learning},
 month = {March},
 note = {arXiv:2203.04462 [cs]},
 publisher = {arXiv},
 title = {Downstream Fairness Caveats with Synthetic Healthcare Data},
 url = {http://arxiv.org/abs/2203.04462},
 urldate = {2023-12-16},
 year = {2022}
}
